エポック数じゃなくて処理時間で各手法の比較を行ってみる？
    １分間でどこまで精度が上がるか？とか

パラメータの更新方法
    W,bのパラメータ更新方法：勾配を効率良く下っていく方法
        確率的勾配法（SGD） ※前章まで使っていた手法
            勾配方向へ一定の距離(係数)だけ進む
            谷のような地形（傾斜が急なところとなだらかなところがある）では効率が悪い
        Momentum
            物理を取り入れた手法
                勾配方向に加速していく
        AdaGrad
            学習が進むにつれて学習係数を小さくしていく
                最初は大胆に最後は詳細に
        Adam
            MomentumとAdaGradを融合させたもの

        TwoLayerNetで比較
            graph_optimizer_loss_loss.png
            graph_optimizer_loss_acc.png
            損失関数(loss)、認識精度(acc)共に下記順で良好な結果が得られた
                Adam > Momentum > AdaGrad > SGD
            書籍の5層Netでは下記順で良好な結果に
                AdaGrad > Adam > Momentum > SGD
            AdaGradは層が深くならないと効果が出ない？

重みの初期値
    0(または均一な値)にしてはいけない
        2層目に同じ値が伝播されるため、層を深くする意味がなくなってしまう
            正しい学習が行われなくなる
    活性化関数の出力をアクティベーションと呼ぶ(0.0〜1.0に分布)
    アクティベーションの分布が適度に広がっていれば効率良く学習を行うことができる
        初期値の設定で分布を広げることができる
        パラメータ数に応じて初期値の分布を小さくする
            Xavier
                sigmoidに適している
                分布 = sqrt(1 / n)
            He
                ReLUに適している
                分布 = sqrt(2 / n)

Batch Normalization
    アクティベーションの分布が適度な広がりを持つように各層で強制的に調整する
        Affine => Batch Norm => ReLUといったフローで実施
        学習が早く進行、初期値に依存しない(初期値にロバスト)、過学習を抑制、といったメリットがある
    データを、平均＝０、分散＝１となるように正則化する
    (?)
        活性化関数ので実施すると効果が変わる？
        絶対値には意味がない？
        なぜ過学習を抑制できる？
            大きな値が発生しなくなるから？

正則化（過学習の抑制）
    過学習の原因
        パラメータ数が多い（表現力の高いモデル）
        訓練データが少ない
    Weight Decay
        Wが大きい＝＞損失「大」として扱う
            L２ノルム = sqrt(w1^2 + w2^2 + ...)　＝ベクトルWの長さ
            L２ノルムを損失関数の値に加算する
        エポックが進む毎にL２ノルムは減少していく
        訓練データの精度が下がる＝過学習が抑制されている
        ニューラルネットワークのモデルが複雑になってくると対応が困難になる
    Dropout
        訓練時に各層のニューロンをランダムに消去する
            信号がそこでストップする
        テスト時は全てのニューロンを使う
            訓練時に消去した割合を乗算する
                ？訓練時の出力は本来の想定よりも小さい？
                ？Batch Normalizationを通すなら乗算は不要では？
        アンサンブル学習(複数のモデルの出力の平均値をとる)に近い
            ランダムに消去＝異なるモデルを学習させている
            消去した割合を乗算＝平均をとる

ハイパーパラメータ
    重み、バイアス以外のパラメータ
    各層のニューロン数、バッチサイズ、パラメータ更新時の学習係数、Weight Decay、など
    テストデータでハイパーパラメータを評価してはいけない
        テストデータに対して過学習してしまうため
    訓練データ、テストデータとは別に検証用データを用意する
        訓練データから20%程度を分離して使うのがもっとも簡単な方法
        データ
            学習用
                訓練データ(x)、テストデータ(t)
            検証用
                訓練データ(x)、テストデータ(t)
    最適化
        良い値の範囲を徐々に絞り込んでいく
            グリッドサーチなどの規則的な探索よりもランダムの法が良い結果になる
                精度に与える影響度合いがハイパーパラメータ毎に異なるから
            対数スケールで範囲を指定
                最初は10^-3から10^3といった感じ
            指定範囲内でランダムに学習した結果を精度が高い順にソートして対数スケールの最大(max)＆最小(min)を取得
                10^minから10^(max+1)の範囲で再度学習・・・この繰り返しで範囲を絞っていく
        ？適切なハイパーパラメータの山は一つだけ？複数あると絞り込めないような…
    やってみること
        TwoLayerで試してみる?
        テストデータで評価するとどうなる？


**** Optimizer比較(ReLU) ****

Optimizer=Adam
loss=2.2964 : train_acc=0.156 : test_acc=0.156
loss=0.2433 : train_acc=0.917 : test_acc=0.920
...
loss=0.0416 : train_acc=0.988 : test_acc=0.971
elapsed_time:55.42701196670532[sec]

Optimizer=Momentum
loss=2.3033 : train_acc=0.080 : test_acc=0.077
loss=0.4172 : train_acc=0.904 : test_acc=0.906
...
loss=0.0516 : train_acc=0.978 : test_acc=0.968
elapsed_time:56.25017595291138[sec]

Optimizer=AdaGrad
loss=2.2389 : train_acc=0.157 : test_acc=0.159
loss=0.2809 : train_acc=0.913 : test_acc=0.916
...
loss=0.1734 : train_acc=0.953 : test_acc=0.948
elapsed_time:60.391401052474976[sec]

Optimizer=SGD
loss=2.3033 : train_acc=0.070 : test_acc=0.067
loss=1.9567 : train_acc=0.550 : test_acc=0.553
...
loss=0.2478 : train_acc=0.919 : test_acc=0.921
elapsed_time:54.726036071777344[sec]

**** weight_init_std(W)が0または1 ****

Optimizer=SGD_W0
loss=2.3025 : train_acc=0.102 : test_acc=0.101
loss=2.3044 : train_acc=0.112 : test_acc=0.114
...
loss=2.2926 : train_acc=0.112 : test_acc=0.114
elapsed_time:52.85869598388672[sec]

Optimizer=SGD_W1
loss=63.9176 : train_acc=0.092 : test_acc=0.101
loss=4.2422 : train_acc=0.674 : test_acc=0.684
...
loss=0.4925 : train_acc=0.832 : test_acc=0.843
elapsed_time:54.79617190361023[sec]

**** Xavier比較(Sigmoid) ****

Optimizer=SGD_Sig_0.01
loss=2.3019 : train_acc=0.112 : test_acc=0.114
loss=2.2958 : train_acc=0.112 : test_acc=0.114
...
loss=0.5834 : train_acc=0.858 : test_acc=0.862
elapsed_time:57.92153787612915[sec]

Optimizer=SGD_Sig_Xavier
loss=2.3436 : train_acc=0.099 : test_acc=0.101
loss=1.8636 : train_acc=0.636 : test_acc=0.649
...
loss=0.4180 : train_acc=0.891 : test_acc=0.896
elapsed_time:53.78064012527466[sec]

**** He比較(ReLU) ****

Optimizer=SGD_0.01
loss=2.3027 : train_acc=0.050 : test_acc=0.047
loss=2.0385 : train_acc=0.514 : test_acc=0.520
...
loss=0.2102 : train_acc=0.921 : test_acc=0.924
elapsed_time:54.20355701446533[sec]

Optimizer=SGD_He
loss=2.3386 : train_acc=0.175 : test_acc=0.175
loss=0.5532 : train_acc=0.847 : test_acc=0.852
...
loss=0.1973 : train_acc=0.934 : test_acc=0.936
elapsed_time:52.72968888282776[sec]

